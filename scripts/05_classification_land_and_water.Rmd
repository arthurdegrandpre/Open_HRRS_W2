---
title: "05_classification_land_and_water"
author: "Arthur de Grandpr√©"
date: "6 mars 2020"
output: 
  html_document: 
    keep_md: yes
    toc: yes
    toc_float: yes
---

This fifth script is used to classify the images, using previous features extraction and image segmentation as described in step 04. In order to perform the supervised classification, a training set will have to be defined manually, using a working GUI version of QGIS V3 or higher. Once the training set is ready, a random forest classification will be performed, to separated submerged and emerging objects.  
  
  This step is required in order to perform sun glint correction. If sun glint correction is not necessary within an image, it can just be skipped directly.

QGIS download : https://www.qgis.org/en/site/

# QGIS and building training sets

In order to train the random forest algorythm, a sufficient amount of objects of known classes must be determined. Since all moisaics are slightly different, the best practice would be to create a training set for every image, which is quite tedious.  

Minimally, you will want to have at least one training set for 4 bands data, and one for 8 bands data from a representative date (more could be necessary depending on the quality of the calibration and atmospheric correction). Since they have different numbers of bands, the same model won't be possible to use for predicting classes from 4bands and 8 bands images.
  
While there is no rule for minimum or maximum amount of objects per class, it is important to distribute the objects well within the different classes. If one class is "water" and the image shows different water masses, enough "water" objects should be defined in every water masses to represent the variations occuring within those water masses, and the broader water class. With more classes, uncertain objects will be easier to control by masking.
  
Since R is a very bad tool for manipulation polygon data, QGIS, an open source GIS suite will be used for this first step.

## QGIS instructions

First of all, look for which images require sun glint correction. This can be done by visually exploring the images in QGIS. (and was done in the previous step (04)

load your first raster and the corresponding segmentation layer into QGIS.  




# R setup

All libraries must be installed prior to loading them. Installing them individually is the best course of action, using the install.packages("nameofthepackage") function.

**important : ** Do not forget to change the tmpdir path in the setup code chunk to a disk with a lot of free space. The raster options prevent the data from being all loaded into the RAM and crashing the process due to lack of memory, thus requiring to write large amounts of intermediate products.

```{r setup, message=F, warning=F}
rm(list=ls()) ; gc()
# install.packages("spatialEco")
# install.packages("randomForest")
# install.packages("party")
library(sf)
library(raster)
library(tidyverse)
library(data.table)
library(sp)
# library(stringr)
library(rgdal)
library(randomForest)
library(spatialEco)
# library(party)
library(velox)
library(GSIF)
# library(doParallel) # could be added at some point to speedup the process
# library(foreach) # could be added at some point to speedup the process

# raster options
rasterOptions(todisk=F) # forces on disk writing instead of in memory
rasterOptions(tmpdir="D:/Arthur/temp") # requires a lot of space for temporary file
# rasterOptions()
rasterOptions(maxmemory = 1e+9)
```

# Random forest classification

## training

Since random forest is a supervised method of classification, it requires training data. Training data must be manually produced using any GIS software, but here, for open source purposes, we select QGIS. Once a proper training set is built (enough samples of enough classes for every image), whole image classification can be performed.

```{r}
input_rasters = "D:/arthur/digitalglobe_archives/feature_extract/" #input BOA images with additionnal features
input_t_obj = "../data/training_sets" #location of training sets.
```


```{r}
rasters = dir(input_rasters, pattern=".tif$", full.names=T)
training_objects = dir(input_t_obj, pattern=".gpkg$", full.names=T)

# rasters
# training_objects

# 8 bands training set
r = brick(rasters[3])
names(r) = c("C","B","G","Y","R","RE","N1","N","LS1","LS2","LS3","LS4","EE","HA1","HA2","HA3","HA4","HA5","HA6","HA7","HA8")

r = dropLayer(r, "LS1")
r = dropLayer(r, "LS2")

s = readOGR(training_objects[2])
s = s[,2:dim(s@data)[2]]

zonal_s = zonal.stats(x = s, y = r, stats = c("min","sd","mean","max"))
s@data = cbind(s@data,zonal_s)

s2 = s@data[is.finite(rowSums(s@data[,2:77])),]

rf4 = randomForest(formula = class~.,
                  data = s2,
                  proximity = T,
                  ntree = 20000,
                  # mtry = 20,
                  # nodesize = 10,
                  importance = T)


plot(rf4)
varImpPlot(rf4)
importance = as.data.frame(importance(rf4))
# importance = importance[order(importance$submerged, decreasing=T),]
head(importance, 10)
rf4

# rf42 = randomForest(formula = class~.,
#                     data = s@data[which(colnames(s@data) %in% c("class",rownames(head(importance,10))))],
#                     proximity = T,
#                     ntree = 5000,
#                     mtry=5,
#                     importance = T)
# 
# plot(rf42)
# varImpPlot(rf42)
# importance = as.data.frame(importance(rf42))
# importance = importance[order(importance$submerged, decreasing=T),]
# head(importance, 10)
# rf42
# 8 bands training set
rm(s);rm(s2);rm(importance);rm(zonal_s)
```

## prediction

### Predictions V2 (fast parrallel)

```{r benchmark, eval=F}

t1 = psp[runif(100,1,length(psp)),]
t2 = psp[runif(1000,1,length(psp)),]
t3 = psp[runif(10000,1,length(psp)),]
t4 = psp[runif(100000,1,length(psp)),] # ACTUALLY CRASHES : USE 10000 AS UPPER LIMIT

zonal_psp = data.frame()

##### benchmark
library("rbenchmark")

benchmark("t100" = {
  rv$extract(t1, fun=function(x){min(x, na.rm=TRUE)})
},
"t1000" = {
  rv$extract(t2, fun=function(x){min(x, na.rm=TRUE)})
},
"t10000" = {
   rv$extract(t3, fun=function(x){min(x, na.rm=TRUE)})
},
# "t100000" = {
  # rv$extract(t4, fun=function(x)(min(x, na.rm=TRUE)))
# },
replications = 3,
columns = c("test","replications","elapsed","relative","user.self","sys.self")
)

# faster up to 10000
```

```{r tiling_function}
make_tiles <- function(j, tile.tbl, 
                          out.path.tif = out.path.tif,
                          out.path.shp = out.path.shp,
                          source = rasters){
  
  out.tif = paste0(out.path.tif, paste0("/T_",j), tile.tbl[j,"ID"], ".tif")
  out.shp = paste0(out.path.shp, paste0("T_",j), tile.tbl[j,"ID"], ".shp")
  
  if(!file.exists(out.tif)){
    m <- readGDAL(rasters[i], offset=unlist(tile.tbl[j,c("offset.y","offset.x")]),
                 region.dim=unlist(tile.tbl[j,c("region.dim.y","region.dim.x")]),
                 output.dim=unlist(tile.tbl[j,c("region.dim.y","region.dim.x")]),
                 silent = TRUE)
    
    if(!all(is.na(m@data[,1]))){
      writeGDAL(m, out.tif, type="Float32", 
                options="COMPRESS=DEFLATE")
      
      #### and write a shapefile sligthly larger ####
      shp_tile = tile.lst[j]
      shp_tile_b = rgeos::gBuffer(shp_tile, width = 25)
      
      writeOGR(SpatialPolygonsDataFrame(shp_tile_b, data=data.frame(1:length(shp_tile_b)), match.ID = F),
               dsn=paste0(out.shp),
               driver="ESRI Shapefile",
               layer=paste0("T_",j))
      
      ogr2ogr(
        src_datasource_name=objects[i],
        dst_datasource_name=out.shp,
        f="ESRI Shapefile",
        clipsrc=out.shp,
        clipdst=out.shp,
        overwrite = T)
      
    }
  }
  }

```

```{r write_tiles, eval=F}
input_rasters = "D:/arthur/digitalglobe_archives/feature_extract/"
rasters = dir(input_rasters, pattern=".tif$", full.names=T)
out.path.tif = ("D:/arthur/digitalglobe_archives/water_mask/tiles/tif/")
out.path.shp = ("D:/arthur/digitalglobe_archives/water_mask/tiles/shp/")

input_objects = "D:/arthur/digitalglobe_archives/segmentation_water/"
objects = dir(input_objects, pattern=".shp$", full.names=T)

i=5

obj = GDALinfo(rasters[i])
tile.lst = getSpatialTiles(obj, block.x=500, return.SpatialPolygons=TRUE)
tile.tbl = getSpatialTiles(obj, block.x=500, return.SpatialPolygons=FALSE)

library(snowfall)

sfInit(parallel=TRUE, cpus=parallel::detectCores())
sfExport("make_tiles","tile.tbl","i","out.path.tif","out.path.shp","rasters","tile.lst","objects")
sfLibrary(rgdal)
sfLibrary(plyr)
sfLibrary(rgeos)
sfLibrary(gdalUtils)

out.lst = sfClusterApplyLB(1:nrow(tile.tbl),
                           function(x){make_tiles(x, tile.tbl ,out.path.tif, out.path.shp, source = rasters)})

sfStop()

```

```{r classification function 2}
classify_tiles = function(j,
                          in.path.r = tifs,
                          in.path.shp = shps,
                          out.path.shp.class = output_shps){
  
out.shp = paste0(paste0(out.path.shp.class,"/",sub(".tif.*","",sub(".*tif/","",tifs))[j],".shp"))  

r = raster::brick(in.path.r[j])
names(r) = c("C","B","G","Y","R","RE","N1","N","LS1","LS2","LS3","LS4","EE","HA1","HA2","HA3","HA4","HA5","HA6","HA7","HA8")

r = dropLayer(r, "LS1")
r = dropLayer(r, "LS2")

pc = rgdal::readOGR(in.path.shp[j])
pc = rgeos::gBuffer(pc, byid=T, width=0)
pc = crop(pc,r)

zonal_s = zonal.stats(x = pc, y = r, stats = c("min","sd","mean","max"))
pc2=pc
pc2@data = cbind(pc@data,zonal_s)

pc2@data = pc2@data[is.finite(rowSums(pc2@data[,2:77])),]
pc2@data$class = "unknown"

pc2@data$class = predict(rf4, pc2@data, type="class")
pc3 = pc
pc3@data = merge(pc@data,pc2@data,all=T)

rgdal::writeOGR(pc3,
                dsn=paste0(out.shp),
                driver="ESRI Shapefile",
                layer=paste0("TC_",j),
                overwrite_layer = T)
}

```


```{r classification function, eval=F}
classify_tiles = function(j,
                          in.path.r = tifs,
                          in.path.shp = shps,
                          out.path.shp.class = output_shps){
  
out.shp = paste0(paste0(out.path.shp.class,"/TC_",j,".shp"))  

r = raster::brick(in.path.r[j])
names(r) = c("C","B","G","Y","R","RE","N1","N","LS1","LS2","LS3","LS4","EE","HA1","HA2","HA3","HA4","HA5","HA6","HA7","HA8")
rv = velox::velox(r)

pc = rgdal::readOGR(in.path.shp[j])

loop_psp = as.data.frame(rv$extract(pc, fun=function(x){min(x, na.rm=TRUE)}))
colnames(loop_psp)=paste0("min.",names(r))
# loop_psp = do.call(data.frame,lapply(loop_psp, function(x) replace(x, is.infinite(x),0)))
pc@data = cbind(loop_psp,pc@data)

loop_psp = as.data.frame(rv$extract(pc, fun=function(x){max(x, na.rm=TRUE)}))
colnames(loop_psp)=paste0("max.",names(r))
# loop_psp = do.call(data.frame,lapply(loop_psp, function(x) replace(x, is.infinite(x),1)))
pc@data = cbind(loop_psp,pc@data)
                  
loop_psp = as.data.frame(rv$extract(pc, fun=function(x){mean(x, na.rm=TRUE)}))
colnames(loop_psp)=paste0("mean.",names(r))
pc@data = cbind(loop_psp,pc@data)
                  
loop_psp = as.data.frame(rv$extract(pc, fun=function(x){sd(x, na.rm=TRUE)}))
colnames(loop_psp)=paste0("sd.",names(r))
# loop_psp[is.na(loop_psp)] = 0
pc@data = cbind(loop_psp,pc@data)

pc2 = sp.na.omit(pc, col.name = "mean.N")
emptycols <- sapply(pc2@data, function (k) all(is.na(k)))
pc2@data <- pc2@data[!emptycols]



pc2@data = pc2@data[is.finite(rowSums(pc2@data[,1:84])),]

pc2@data$class = "unknown"
test = pc2@data
test2 = predict(rf4, pc2@data, type="vote")
summary(test2)
summary(test2)

test3 = Filter(function(x)!all(is.na(x)), test)



pc2@data$class = predict(rf4, newdata=pc2@data, type = "vote")

summary(pc2@data)
rf4$call

rgdal::writeOGR(pc,
                dsn=paste0(out.shp),
                driver="ESRI Shapefile",
                layer=paste0("TC_",j),
                overwrite_layer = T)
}

```


```{r classify_tiles_2}

tifs = dir("D:/arthur/digitalglobe_archives/water_mask/tiles/", full.names=T, recursive=T, pattern=".tif$")
shps = dir("D:/arthur/digitalglobe_archives/water_mask/tiles/shp/", full.names=T, pattern=".shp$")

input_objects = "D:/arthur/digitalglobe_archives/segmentation_water/"
objects = dir(input_objects, pattern=".shp$", full.names=T)
i=5

output_shps = "D:/arthur/digitalglobe_archives/water_mask/tiles/shp/classified/"

library(snowfall)

sfInit(parallel=TRUE, cpus=parallel::detectCores()-1)
sfExport("tifs","shps","output_shps","classify_tiles","rf4")
sfLibrary(rgdal)
sfLibrary(raster)
sfLibrary(velox)
sfLibrary(randomForest)
sfLibrary(spatialEco)


out.lst = sfClusterApplyLB(1:length(tifs),
                           function(x){classify_tiles(x, tifs, shps, output_shps)})

sfStop()
```


```{r classify_tiles, eval=F}

i=5

zonal_psp = data.frame()

tifs = dir("D:/arthur/digitalglobe_archives/water_mask/tiles/", full.names=T, recursive=T, pattern=".tif$")
shps = dir("D:/arthur/digitalglobe_archives/water_mask/tiles/", full.names=T, recursive=T, pattern=".shp$")

input_objects = "D:/arthur/digitalglobe_archives/segmentation_water/"
objects = dir(input_objects, pattern=".shp$", full.names=T)

library(doParallel)
library(parallel)
cores = detectCores()
cl = makeCluster(cores[1])
registerDoParallel(cl)

zonal_all = foreach(j=1:length(shps),.combine=rbind) %dopar% {

r = raster::brick(tifs[j])
names(r) = c("C","B","G","Y","R","RE","N1","N","LS1","LS2","LS3","LS4","EE","HA1","HA2","HA3","HA4","HA5","HA6","HA7","HA8")
rv = velox::velox(r)

pc = rgdal::readOGR(shps[j])


loop_psp = as.data.frame(rv$extract(pc, fun=function(x){min(x, na.rm=TRUE)}))
colnames(loop_psp)=paste0("min.",names(r))

extract_psp = as.data.frame(rv$extract(pc, fun=function(x){max(x, na.rm=TRUE)}))
colnames(extract_psp)=paste0("max.",names(r))
loop_psp = cbind(loop_psp, extract_psp)
                  
extract_psp = as.data.frame(rv$extract(pc, fun=function(x){mean(x, na.rm=TRUE)}))
colnames(extract_psp)=paste0("mean.",names(r))
loop_psp = cbind(loop_psp, extract_psp)
                  
extract_psp = as.data.frame(rv$extract(pc, fun=function(x){sd(x, na.rm=TRUE)}))
colnames(extract_psp)=paste0("sd.",names(r))
loop_psp = cbind(loop_psp, extract_psp)

row.names(loop_psp) = pc@data$DN
loop_psp = loop_psp[!apply(is.na(loop_psp) | loop_psp =="", 1, all),]

zonal_psp = rbind(loop_psp, zonal_psp)
return(loop_psp)
}

stopCluster(cl)


psp = readOGR(objects[i])
psp2 = psp

psp2@data = merge(psp2@data,zonal_all, by.x=1, by.y=0)

# psp2@data = psp2@data[order(as.numeric(psp2@data$Row.names)),]
psp2@data$class="unknown"
psp2 = sp.na.omit(psp2)

psp2@data$class = predict(rf4, newdata=psp2@data, type = "class")
colnames(psp2@data)[1]="rowname"
psp2@data$rowname=as.numeric(psp2@data$rowname)
writeOGR(psp2, dsn="D:/arthur/digitalglobe_archives/water_mask/landmask_2013_50_P002.gpkg", layer="classification",driver="GPKG", overwrite_layer = T)
```

```{r, eval=F}
library(rgdal)
library(GSIF)
library(foreach)
library(doParallel)

tiles.tbl = getSpatialTiles(psp, block.x = 500)
# plot(r[[1]]);plot(test,add=T)

cores = detectCores()
# cl = makeCluster(cores[1]-2)
cl = makeCluster(3) #until memory problem is resolved
registerDoParallel(cl)

zonal_all = foreach(i=1:length(tiles.tbl),.combine=rbind) %dopar% {
# i=35
  gc()
zonal_psp = data.frame()
rc = raster::crop(r, tiles.tbl[i])
if(all(is.na(raster::getValues(rc[[1]])))){return(NA)}
pc = raster::crop(psp, tiles.tbl[i])
rv = velox::velox(rc)

extract_psp = as.data.frame(rv$extract(pc, fun=function(x){min(x, na.rm=TRUE)}))
colnames(extract_psp)=paste0("min.",names(r))
zonal_psp = rbind(zonal_psp, extract_psp)

extract_psp = as.data.frame(rv$extract(pc, fun=function(x){max(x, na.rm=TRUE)}))
colnames(extract_psp)=paste0("max.",names(r))
zonal_psp = cbind(zonal_psp, extract_psp)
                  
extract_psp = as.data.frame(rv$extract(pc, fun=function(x){mean(x, na.rm=TRUE)}))
colnames(extract_psp)=paste0("mean.",names(r))
zonal_psp = cbind(zonal_psp, extract_psp)
                  
extract_psp = as.data.frame(rv$extract(pc, fun=function(x){sd(x, na.rm=TRUE)}))
colnames(extract_psp)=paste0("sd.",names(r))
zonal_psp = cbind(zonal_psp, extract_psp)

return(zonal_psp)
}

stopCluster(cl)

psp2 = psp
psp2@data = merge(psp2@data,zonal_all, by.x=0, by.y=0)
psp2@data = psp2@data[order(as.numeric(psp2@data$Row.names)),]
# psp2@data$class="unknown" 
psp2 = sp.na.omit(psp2)
                  
psp2@data$class = predict(rf4, newdata=psp2@data, type = "class")
colnames(psp2@data)[1]="rowname"
psp2@data$rowname=as.numeric(psp2@data$rowname)
writeOGR(psp2, dsn="D:/arthur/digitalglobe_archives/water_mask/landmask_2013_50_P002.gpkg", layer="classification",driver="GPKG", overwrite_layer = T)

```

```{r, eval=F}
                  input_rasters = "D:/arthur/digitalglobe_archives/feature_extract/"
                  rasters = dir(input_rasters, pattern=".tif$", full.names=T)
                  input_objects = "D:/arthur/digitalglobe_archives/segmentation_water/"
                  output_rasters = "D:/arthur/digitalglobe_archives/water_mask/"
                  
                  i=1  
                  r = brick(rasters[i])
                  names(r) = c("C","B","G","Y","R","RE","N1","N","LS1","LS2","LS3","LS4","EE","HA1","HA2","HA3","HA4","HA5","HA6","HA7","HA8")
                  objects = dir(input_objects, pattern=".shp$", full.names=T)
                  
                  psp = readOGR(objects[i])
                  psp@data$class = "unknown"
                  
                  psp = psp[,2:dim(psp@data)[2]]
                  
                  rv = velox(r)
                  
                  zonal_psp = data.frame()
                  
                  extract_psp = rv$extract(psp, fun=function(x){min(x, na.rm=TRUE)})
                  extract_psp = as.data.frame(extract_psp)
                  colnames(extract_psp)=paste0("min.",names(r))
                  zonal_psp = rbind(zonal_psp, extract_psp)
                  
                  extract_psp = as.data.frame(rv$extract(psp, fun=function(x){max(x, na.rm=TRUE)}))
                  colnames(extract_psp)=paste0("max.",names(r))
                  zonal_psp = cbind(zonal_psp, extract_psp)
                  
                  extract_psp = as.data.frame(rv$extract(psp, fun=function(x){mean(x, na.rm=TRUE)}))
                  colnames(extract_psp)=paste0("mean.",names(r))
                  zonal_psp = cbind(zonal_psp, extract_psp)
                  
                  extract_psp = as.data.frame(rv$extract(psp, fun=function(x){sd(x, na.rm=TRUE)}))
                  colnames(extract_psp)=paste0("sd.",names(r))
                  zonal_psp = cbind(zonal_psp, extract_psp)
                  
                  psp2 = psp
                  psp2@data = cbind(psp@data,zonal_psp)
                  # psp2@data$class="unknown" 
                  psp2 = sp.na.omit(psp2)
                  
                  psp2@data$class = predict(rf4, newdata=psp2@data, type = "class")
writeOGR(psp2, dsn="D:/arthur/digitalglobe_archives/water_mask/class_test_2019_2.gpkg", layer="classification",driver="GPKG", overwrite_layer = T)

# note : 2017 was overwritten by 2019.... 
# unique(psp2@data$class)

# submerged = psp2[psp2@data$class %in% c("water_green","water_brown","water_dark","water_shadow","water_shallow"),] # 2011
# submerged = psp2[psp2@data$class %in% c("water_blue","water_clear","water_shallow","veg_submerged"),] #2013 40p2
psp2 = readOGR("D:/arthur/digitalglobe_archives/water_mask/class_test_2019_2.gpkg")
# submerged = psp2[psp2@data$class %in% c("water_blue","water_dark","water_brown","water_white","water_shallow","veg_submerged"),] #2017

submerged = psp2[psp2@data$class %in% c("water_shallow","water_deep","water_clear","shallow_water","veg_submerged"),] #2019
submerged_mask = st_union(st_as_sf(submerged))
submerged_mask2 = st_cast(submerged_mask,"POLYGON")
smask = as_Spatial(submerged_mask2)
water = mask(r[[1:8]], smask)

writeRaster(water, paste0(output_rasters,str_sub(rasters[i],-35)), overwrite=T)

```

# Glint correction

```{r hochberg_v3, eval=F}
### this 3rd implementation of the hochberg glint correction algorythm includes grouping for band TDI.
### EX: for wv02, it seems coastal is timed with NIR2, yellow is timed with red edge, and RGB + NIR1 are timed together.

# imgs = dir("D:/arthur/digitalglobe_archives/water_mask/",full.names=T, pattern=".tif$")
imgs = dir("D:/arthur/digitalglobe_archives/destripe/",full.names=T, pattern=".tif$")
deep_zones = dir("D:/arthur/digitalglobe_archives/water_mask/deep_zones",full.names=T, pattern=".shp$")[7:8]
tdi_table = read.csv("../data/radiometric_calibration_parameters.csv")

# for(i in 1:length(imgs)){
i=2
  print(paste0("image ",i," out of ",length(imgs)))

  tdi_img = tdi_table[str_detect(as.character(imgs[i]), as.character(tdi_table$id_part)),]
  tdi_img$bands = gsub(" ","",tdi_img$bands)
  tdi_img = subset(tdi_img, tdi_img$bands != "P")
  tdi_img$tdi = as.numeric(tdi_img$tdi)
  tdis = unique(tdi_img$tdi)

  
  r = brick(imgs[i])
  if(nlayers(r)==8){names(r) = c("C","B","G","Y","R","RE","N","N2")}
  if(nlayers(r)==4){names(r) = c("B","G","R","N")}
  
  dz = readOGR(deep_zones[i])
  
  for(k in seq_along(tdis)){
    if(length(tdis)==4){
    tdi_img[tdi_img$tdi==tdis[2],]$tdi = tdis[4]
    }
  
  bands = subset(tdi_img$bands, tdi_img$tdi==tdis[k])
    
    rs = subset(r,tdi_img[tdi_img$tdi==tdis[k],]$bands)
    dz = spTransform(dz, crs(rs))
    rdz = readAll(crop(rs,dz))
  
  max.coords = xyFromCell(rdz,which.max(rdz[[nlayers(rdz)]]))
  max.coords.pixel = SpatialPoints(max.coords)
  max.pixel = raster::extract(rdz,max.coords.pixel)
  
  min.coords = xyFromCell(rdz,which.min(rdz[[nlayers(rdz)]]))
  min.coords.pixel = SpatialPoints(t(as.data.frame(min.coords[1,])))
  min.pixel = raster::extract(rdz,min.coords.pixel)
  
  Lg_nir = max.pixel[[nlayers(rs)]] - min.pixel[[nlayers(rs)]]
  
  fg = (rs[[nlayers(rs)]] - min.pixel[[nlayers(rs)]]) / Lg_nir
  
  Lg = list()
  
  r2 = rs

  for(b in 1:nlayers(rs)){
    print(b)
    Lg[[b]] = max.pixel[[b]] - min.pixel[[b]]
    r2[[b]] = rs[[b]] - fg * Lg[[b]]
  }
  names(r2)=names(rs)
  
  if(k==1){r3=r2} else{r3 = addLayer(r3,r2)}
  
  if(k==length(tdis)){
    r3 = r3[[names(r)]]
    # writeRaster(r3, paste0("D:/arthur/digitalglobe_archives/deglint_hochberg/hochberg3/",str_sub(imgs[i],-46)), overwrite=T)
    writeRaster(r3, paste0("D:/arthur/digitalglobe_archives/deglint_hochberg/hochberg_destripe/",str_sub(imgs[i],-46)), overwrite=T)
  } 
  }
# }
length(tdis)
```


```{r hochbergv2, eval=F}
# install.packages("mapview")
# install.packages("mapedit")
# library(mapview)
# library(mapedit)

imgs = dir("D:/arthur/digitalglobe_archives/water_mask/",full.names=T, pattern=".tif$")
deep_zones = dir("D:/arthur/digitalglobe_archives/water_mask/deep_zones",full.names=T, pattern=".shp$")


# for(i in 1:length(imgs)){
i=7
  print(paste0("image ",i," out of ",length(imgs)))

  r = brick(imgs[i])
  # summary(r)
  # r = (r>=0)*r
  # summary(r)
  # plot(r)
  dz = readOGR(deep_zones[i])

  #### 2019 only
  dz = spTransform(dz, crs(r))
  ####
  
  rdz = readAll(crop(r,dz))
  
  max.coords = xyFromCell(rdz,which.max(rdz[[nlayers(rdz)]]))
  max.coords.pixel = SpatialPoints(max.coords)
  max.pixel = raster::extract(rdz,max.coords.pixel)
  
  min.coords = xyFromCell(rdz,which.min(rdz[[nlayers(rdz)]]))
  min.coords.pixel = SpatialPoints(t(as.data.frame(min.coords[1,])))
  min.pixel = raster::extract(rdz,min.coords.pixel)
  
  Lg_nir = max.pixel[[nlayers(r)]] - min.pixel[[nlayers(r)]]
  
  fg = (r[[nlayers(r)]] - min.pixel[[nlayers(r)]]) / Lg_nir # or should this use the min NIR value from the whole scene?
  # plot(fg)
  
  Lg = list()
  
  r2 = r
  
  for(b in 1:nlayers(r)){
    print(b)
    Lg[[b]] = max.pixel[[b]] - min.pixel[[b]]
    r2[[b]] = r[[b]] - fg * Lg[[b]]
  }
  
  writeRaster(r2, paste0("D:/arthur/digitalglobe_archives/deglint_hochberg/hochberg2/",str_sub(imgs[i],-46)), overwrite=T)
  
  # }

```

```{r hochbergv1, eval=F}

imgs = dir("//Glaciolab/homes/degranda/MFFP/digitalglobe_archives/water_mask/",full.names=T, pattern=".tif$")

# for(i in 1:length(imgs)){

i=6
print(paste0("image ",i," of ",length(imgs)))
  r = brick(imgs[i])

Lg_nir = quantile(r[[nlayers(r)]], probs=c(0.99)) - min(values(r[[nlayers(r)]]), na.rm = T) # uses the 99.999% quantile instead of max value

## added removed 2 9 for 2013 050_p002 because of adjacency effect

fg = (r[[nlayers(r)]]-min(values(r[[nlayers(r)]]), na.rm = T)) / Lg_nir

Lg=list()

r2 = r
for(b in 1:nlayers(r)){
  print(b)
  Lg[[b]] = max(values(r[[b]]), na.rm = T) - min(values(r[[b]]), na.rm = T)
  r2[[b]] = r[[b]] - fg * Lg[[b]]
  }  
# 
# L=0.5
# r$wavi = (1+L)*((r[[8]]-r[[2]])/(r[[8]]+r[[2]]+L))
# r2$wavi = (1+L)*((r2[[8]]-r2[[2]])/(r2[[8]]+r2[[2]]+L))
# 
# plot(r);plot(r2)

writeRaster(r2, paste0("//Glaciolab/homes/degranda/MFFP/digitalglobe_archives/deglint_hochberg/",str_sub(imgs[i],-46)), overwrite=T)
# }

```

